SYSCALL_DEFINE5() <SYSCALL_DEFINE5 (perf_event_open, struct perf_event_attr __user *, attr_uptr, pid_t, pid, int, cpu, int, group_fd, unsigned long, flags) at core.c:8268>:
    perf_copy_attr() <int perf_copy_attr (struct perf_event_attr __user *uattr, struct perf_event_attr *attr) at core.c:8023>:
        access_ok()
        get_user()
        copy_from_user()
        perf_paranoid_kernel()
        capable()
        perf_reg_validate()
        arch_perf_have_user_stack_dump()
        IS_ALIGNED()
        put_user()
    perf_paranoid_kernel()
    capable()
    get_unused_fd_flags()
    perf_fget_light() <inline int perf_fget_light (int fd, struct fd *p) at core.c:4235>:
        fdget()
        fdput()
    find_lively_task_by_vpid() <struct task_struct *find_lively_task_by_vpid (pid_t vpid) at core.c:3405>:
        rcu_read_lock()
        find_task_by_vpid()
        get_task_struct()
        rcu_read_unlock()
        ERR_PTR()
        ptrace_may_access()
        put_task_struct()
    IS_ERR()
    PTR_ERR()
    get_online_cpus()
    perf_event_alloc() <struct perf_event *perf_event_alloc (struct perf_event_attr *attr, int cpu, struct task_struct *task, struct perf_event *group_leader, struct perf_event *parent_event, perf_overflow_handler_t overflow_handler, void *context, int cgroup_fd) at core.c:7877>:
        ERR_PTR()
        kzalloc()
        mutex_init()
        INIT_LIST_HEAD()
        INIT_HLIST_NODE()
        init_waitqueue_head()
        init_irq_work()
        perf_pending_event() <void perf_pending_event (struct irq_work *entry) at core.c:4986>:
            container_of()
            perf_swevent_get_recursion_context() <int perf_swevent_get_recursion_context (void) at core.c:6685>:
                this_cpu_ptr()
                get_recursion_context()
            perf_event_disable_local() <void perf_event_disable_local (struct perf_event *event) at core.c:1841>:
                event_function_local() <void event_function_local (struct perf_event *event, event_f func, void *data) at core.c:243>:
                    event_function() <int event_function (void *info) at core.c:197>:
                        WARN_ON_ONCE()
                        irqs_disabled()
                        perf_ctx_lock() <void perf_ctx_lock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:146>:
                            raw_spin_lock()
                        perf_ctx_unlock() <void perf_ctx_unlock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:154>:
                            raw_spin_unlock()
                    WARN_ON_ONCE()
            perf_event_wakeup() <void perf_event_wakeup (struct perf_event *event) at core.c:4976>:
                ring_buffer_wakeup() <void ring_buffer_wakeup (struct perf_event *event) at core.c:4573>:
                    rcu_read_lock()
                    rcu_dereference()
                    list_for_each_entry_rcu()
                    wake_up_all()
                    rcu_read_unlock()
                kill_fasync()
                perf_event_fasync() <inline struct fasync_struct **perf_event_fasync (struct perf_event *event) at core.c:4968>
            perf_swevent_put_recursion_context() <inline void perf_swevent_put_recursion_context (int rctx) at core.c:6693>:
                this_cpu_ptr()
                put_recursion_context()
        atomic_long_set()
        get_pid_ns()
        task_active_pid_ns()
        atomic64_inc_return()
        local_clock()
        perf_event__state_init() <inline void perf_event__state_init (struct perf_event *event) at core.c:1402>:
        local64_set()
        has_branch_stack()
        perf_cgroup_connect() <inline int perf_cgroup_connect (pid_t pid, struct perf_event *event, struct perf_event_attr *attr, struct perf_event *group_leader) at core.c:859>:
            fdget()
            css_tryget_online_from_dir()
            IS_ERR()
            PTR_ERR()
            container_of()
            perf_detach_cgroup() <inline void perf_detach_cgroup (struct perf_event *event) at core.c:828>:
                css_put()
            fdput()
        perf_init_event() <struct pmu *perf_init_event (struct perf_event *event) at core.c:7775>:
            srcu_read_lock()
            rcu_read_lock()
            idr_find()
            rcu_read_unlock()
            perf_try_init_event() <int perf_try_init_event (struct pmu *pmu, struct perf_event *event) at core.c:7745>:
                try_module_get()
                perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
                    rcu_read_lock()
                    ACCESS_ONCE()
                    atomic_inc_not_zero()
                    rcu_read_unlock()
                    mutex_lock_nested()
                    mutex_unlock()
                    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                        atomic_dec_and_test()
                        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 96)
                        put_task_struct()
                        call_rcu()
                        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                            container_of()
                            kfree()
                BUG_ON()
                perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
                    mutex_unlock()
                    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                        atomic_dec_and_test()
                        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 107)
                        put_task_struct()
                        call_rcu()
                        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                            container_of()
                            kfree()
                module_put()
            ERR_PTR()
            list_for_each_entry_rcu()
            srcu_read_unlock()
        IS_ERR()
        PTR_ERR()
        exclusive_event_init() <int exclusive_event_init (struct perf_event *event) at core.c:3621>:
            atomic_inc_unless_negative()
            atomic_dec_unless_positive()
        get_callchain_buffers()
        exclusive_event_destroy() <void exclusive_event_destroy (struct perf_event *event) at core.c:3652>:
            atomic_dec()
            atomic_inc()
        module_put()
        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
        perf_detach_cgroup() <inline void perf_detach_cgroup (struct perf_event *event) at core.c:828>:
            css_put()
        put_pid_ns()
        kfree()
    is_sampling_event()
    account_event() <void account_event (struct perf_event *event) at core.c:7819>:
        atomic_inc()
        atomic_inc_return()
        tick_nohz_full_kick_all()
        has_branch_stack()
        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
        atomic_inc_not_zero()
        mutex_lock()
        atomic_read()
        static_branch_enable()
        synchronize_sched()
        mutex_unlock()
        account_event_cpu() <void account_event_cpu (struct perf_event *event, int cpu) at core.c:7810>:
            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
            atomic_inc()
            per_cpu()
    perf_event_set_clock() <int perf_event_set_clock (struct perf_event *event, clockid_t clk_id) at core.c:8223>:
    is_software_event()
    find_get_context() <struct perf_event_context *find_get_context (struct pmu *pmu, struct task_struct *task, struct perf_event *event) at core.c:3438>:
        perf_paranoid_cpu()
        capable()
        ERR_PTR()
        cpu_online()
        per_cpu_ptr()
        get_ctx() <void get_ctx (struct perf_event_context *ctx) at core.c:1015>:
            WARN_ON()
            atomic_inc_not_zero()
        kzalloc()
        perf_lock_task_context() <struct perf_event_context *perf_lock_task_context (struct task_struct *task, int ctxn, unsigned long *flags) at core.c:1199>:
            local_irq_save()
            rcu_read_lock()
            rcu_dereference()
            raw_spin_lock()
            raw_spin_unlock()
            rcu_read_unlock()
            local_irq_restore()
            atomic_inc_not_zero()
            WARN_ON_ONCE()
        unclone_ctx()
        raw_spin_unlock_irqrestore()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 175)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
        alloc_perf_context() <struct perf_event_context *alloc_perf_context (struct pmu *pmu, struct task_struct *task) at core.c:3386>:
            kzalloc()
            get_task_struct()
        mutex_lock()
        rcu_assign_pointer()
        mutex_unlock()
        unlikely()
        kfree()
    put_task_struct()
    perf_event_set_output() <int perf_event_set_output (struct perf_event *event, struct perf_event *output_event) at core.c:8154>:
        has_aux()
        mutex_lock()
        atomic_read()
        ring_buffer_get() <struct ring_buffer *ring_buffer_get (struct perf_event *event) at core.c:4586>:
            rcu_read_lock()
            rcu_dereference()
            atomic_inc_not_zero()
            rcu_read_unlock()
        ring_buffer_attach() <void ring_buffer_attach (struct perf_event *event, struct ring_buffer *rb) at core.c:4527>:
            WARN_ON_ONCE()
            spin_lock_irqsave()
            list_del_rcu()
            spin_unlock_irqrestore()
            get_state_synchronize_rcu()
            cond_synchronize_rcu()
            list_add_rcu()
            rcu_assign_pointer()
            ring_buffer_put() <void ring_buffer_put (struct ring_buffer *rb) at core.c:4601>:
                atomic_dec_and_test()
                WARN_ON_ONCE()
                list_empty()
                call_rcu()
            wake_up_all()
        mutex_unlock()
    anon_inode_getfile()
    mutex_lock_double() <void mutex_lock_double (struct mutex *a, struct mutex *b) at core.c:8214>:
        swap()
        mutex_lock()
        mutex_lock_nested()
    mutex_lock()
    perf_event_validate_size() <bool perf_event_validate_size (struct perf_event *event) at core.c:1499>:
        perf_event__id_header_size() <void perf_event__id_header_size (struct perf_event *event) at core.c:1472>:
    exclusive_event_installable() <bool exclusive_event_installable (struct perf_event *event, struct perf_event_context *ctx) at core.c:3677>:
        list_for_each_entry()
        exclusive_event_match() <bool exclusive_event_match (struct perf_event *e1, struct perf_event *e2) at core.c:3666>:
    WARN_ON_ONCE()
    perf_remove_from_context() <void perf_remove_from_context (struct perf_event *event, unsigned long flags) at core.c:1785>:
        lockdep_assert_held()
        event_function_call() <void event_function_call (struct perf_event *event, event_f func, void *data) at core.c:255>:
            READ_ONCE()
            lockdep_assert_held()
            cpu_function_call() <int cpu_function_call (int cpu, remote_function_f func, void *info) at core.c:126>:
                smp_call_function_single()
                remote_function() <void remote_function (void *data) at core.c:61>:
                    task_cpu()
                    smp_processor_id()
            event_function() <int event_function (void *info) at core.c:197>:
                WARN_ON_ONCE()
                irqs_disabled()
                perf_ctx_lock() <void perf_ctx_lock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:146>:
                    raw_spin_lock()
                perf_ctx_unlock() <void perf_ctx_unlock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:154>:
                    raw_spin_unlock()
            task_function_call() <int task_function_call (struct task_struct *p, remote_function_f func, void *info) at core.c:98>:
                smp_call_function_single()
                task_cpu()
                remote_function() <void remote_function (void *data) at core.c:61>:
                    task_cpu()
                    smp_processor_id()
            raw_spin_lock_irq()
            raw_spin_unlock_irq()
    list_for_each_entry()
    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
        atomic_dec_and_test()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 255)
        put_task_struct()
        call_rcu()
        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
            container_of()
            kfree()
    synchronize_rcu()
    perf_event__state_init() <inline void perf_event__state_init (struct perf_event *event) at core.c:1402>:
    perf_install_in_context() <void perf_install_in_context (struct perf_event_context *ctx, struct perf_event *event, int cpu) at core.c:2178>:
        READ_ONCE()
        lockdep_assert_held()
        cpu_function_call() <int cpu_function_call (int cpu, remote_function_f func, void *info) at core.c:126>:
            smp_call_function_single()
            remote_function() <void remote_function (void *data) at core.c:61>:
                task_cpu()
                smp_processor_id()
        WARN_ON_ONCE()
        task_cpu()
        raw_spin_lock_irq()
        raw_spin_unlock_irq()
    get_ctx() <void get_ctx (struct perf_event_context *ctx) at core.c:1015>:
        WARN_ON()
        atomic_inc_not_zero()
    perf_event__header_size() <void perf_event__header_size (struct perf_event *event) at core.c:1465>:
    perf_event__id_header_size() <void perf_event__id_header_size (struct perf_event *event) at core.c:1472>:
    perf_unpin_context() <void perf_unpin_context (struct perf_event_context *ctx) at core.c:1268>:
        raw_spin_lock_irqsave()
        raw_spin_unlock_irqrestore()
    mutex_unlock()
    put_online_cpus()
    list_add_tail()
    fdput()
    fd_install()
    fput()
    free_event() <void free_event (struct perf_event *event) at core.c:3740>:
        WARN()
        atomic_long_cmpxchg()
        atomic_long_read()
    put_unused_fd()
perf_bp_event() <void perf_bp_event (struct perf_event *bp, void *data) at core.c:7140>:
    perf_sample_data_init()
    perf_exclude_event() <int perf_exclude_event (struct perf_event *event, struct pt_regs *regs) at core.c:6577>:
        user_mode()
    perf_swevent_event() <void perf_swevent_event (struct perf_event *event, u64 nr, struct perf_sample_data *data, struct pt_regs *regs) at core.c:6548>:
        local64_add()
        is_sampling_event()
        perf_swevent_overflow() <void perf_swevent_overflow (struct perf_event *event, u64 overflow, struct perf_sample_data *data, struct pt_regs *regs) at core.c:6522>:
            perf_swevent_set_period() <u64 perf_swevent_set_period (struct perf_event *event) at core.c:6499>:
                local64_read()
                div64_u64()
                local64_cmpxchg()
        local64_add_negative()
perf_cpu_time_max_percent_handler() <int perf_cpu_time_max_percent_handler (struct ctl_table *table, int write, void __user *buffer, size_t *lenp, loff_t *ppos) at core.c:403>:
    proc_dointvec()
    update_perf_cpu_limits() <void update_perf_cpu_limits (void) at core.c:374>:
        do_div()
        ACCESS_ONCE()
perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
perf_event_attrs() <const struct perf_event_attr *perf_event_attrs (struct perf_event *event) at core.c:9023>:
    ERR_PTR()
perf_event_aux_event() <void perf_event_aux_event (struct perf_event *event, unsigned long head, unsigned long size, u64 flags) at core.c:6169>:
    perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
    perf_output_begin()
    perf_output_put()
    perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
    perf_output_end()
perf_event_can_stop_tick() <bool perf_event_can_stop_tick (void) at core.c:3116>:
    atomic_read()
perf_event_comm() <void perf_event_comm (struct task_struct *task, bool exec) at core.c:5902>:
    atomic_read()
    perf_event_comm_event() <void perf_event_comm_event (struct perf_comm_event *comm_event) at core.c:5883>:
        strlcpy()
        ALIGN()
        perf_event_aux() <void perf_event_aux (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5689>:
            perf_event_aux_task_ctx() <void perf_event_aux_task_ctx (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5678>:
                rcu_read_lock()
                preempt_disable()
                perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                    list_for_each_entry_rcu()
                    event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                        smp_processor_id()
                        perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                            cgroup_is_descendant()
                        pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                preempt_enable()
                rcu_read_unlock()
            rcu_read_lock()
            list_for_each_entry_rcu()
            get_cpu_ptr()
            perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                list_for_each_entry_rcu()
                event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                    smp_processor_id()
                    perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                        cgroup_is_descendant()
                    pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
            rcu_dereference()
            put_cpu_ptr()
            rcu_read_unlock()
        perf_event_comm_output() <void perf_event_comm_output (struct perf_event *event, void *data) at core.c:5850>:
            perf_event_comm_match() <int perf_event_comm_match (struct perf_event *event) at core.c:5845>:
            perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
            perf_output_begin()
            perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                task_tgid_nr_ns()
            perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                task_pid_nr_ns()
            perf_output_put()
            perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
            perf_output_end()
perf_event_create_kernel_counter() <struct perf_event *perf_event_create_kernel_counter (struct perf_event_attr *attr, int cpu, struct task_struct *task, perf_overflow_handler_t overflow_handler, void *context) at core.c:8642>:
    perf_event_alloc() <struct perf_event *perf_event_alloc (struct perf_event_attr *attr, int cpu, struct task_struct *task, struct perf_event *group_leader, struct perf_event *parent_event, perf_overflow_handler_t overflow_handler, void *context, int cgroup_fd) at core.c:7877>:
        ERR_PTR()
        kzalloc()
        mutex_init()
        INIT_LIST_HEAD()
        INIT_HLIST_NODE()
        init_waitqueue_head()
        init_irq_work()
        perf_pending_event() <void perf_pending_event (struct irq_work *entry) at core.c:4986>:
            container_of()
            perf_swevent_get_recursion_context() <int perf_swevent_get_recursion_context (void) at core.c:6685>:
                this_cpu_ptr()
                get_recursion_context()
            perf_event_disable_local() <void perf_event_disable_local (struct perf_event *event) at core.c:1841>:
                event_function_local() <void event_function_local (struct perf_event *event, event_f func, void *data) at core.c:243>:
                    event_function() <int event_function (void *info) at core.c:197>:
                        WARN_ON_ONCE()
                        irqs_disabled()
                        perf_ctx_lock() <void perf_ctx_lock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:146>:
                            raw_spin_lock()
                        perf_ctx_unlock() <void perf_ctx_unlock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:154>:
                            raw_spin_unlock()
                    WARN_ON_ONCE()
            perf_event_wakeup() <void perf_event_wakeup (struct perf_event *event) at core.c:4976>:
                ring_buffer_wakeup() <void ring_buffer_wakeup (struct perf_event *event) at core.c:4573>:
                    rcu_read_lock()
                    rcu_dereference()
                    list_for_each_entry_rcu()
                    wake_up_all()
                    rcu_read_unlock()
                kill_fasync()
                perf_event_fasync() <inline struct fasync_struct **perf_event_fasync (struct perf_event *event) at core.c:4968>
            perf_swevent_put_recursion_context() <inline void perf_swevent_put_recursion_context (int rctx) at core.c:6693>:
                this_cpu_ptr()
                put_recursion_context()
        atomic_long_set()
        get_pid_ns()
        task_active_pid_ns()
        atomic64_inc_return()
        local_clock()
        perf_event__state_init() <inline void perf_event__state_init (struct perf_event *event) at core.c:1402>:
        local64_set()
        has_branch_stack()
        perf_cgroup_connect() <inline int perf_cgroup_connect (pid_t pid, struct perf_event *event, struct perf_event_attr *attr, struct perf_event *group_leader) at core.c:859>:
            fdget()
            css_tryget_online_from_dir()
            IS_ERR()
            PTR_ERR()
            container_of()
            perf_detach_cgroup() <inline void perf_detach_cgroup (struct perf_event *event) at core.c:828>:
                css_put()
            fdput()
        perf_init_event() <struct pmu *perf_init_event (struct perf_event *event) at core.c:7775>:
            srcu_read_lock()
            rcu_read_lock()
            idr_find()
            rcu_read_unlock()
            perf_try_init_event() <int perf_try_init_event (struct pmu *pmu, struct perf_event *event) at core.c:7745>:
                try_module_get()
                perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
                    rcu_read_lock()
                    ACCESS_ONCE()
                    atomic_inc_not_zero()
                    rcu_read_unlock()
                    mutex_lock_nested()
                    mutex_unlock()
                    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                        atomic_dec_and_test()
                        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 436)
                        put_task_struct()
                        call_rcu()
                        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                            container_of()
                            kfree()
                BUG_ON()
                perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
                    mutex_unlock()
                    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                        atomic_dec_and_test()
                        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 447)
                        put_task_struct()
                        call_rcu()
                        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                            container_of()
                            kfree()
                module_put()
            ERR_PTR()
            list_for_each_entry_rcu()
            srcu_read_unlock()
        IS_ERR()
        PTR_ERR()
        exclusive_event_init() <int exclusive_event_init (struct perf_event *event) at core.c:3621>:
            atomic_inc_unless_negative()
            atomic_dec_unless_positive()
        get_callchain_buffers()
        exclusive_event_destroy() <void exclusive_event_destroy (struct perf_event *event) at core.c:3652>:
            atomic_dec()
            atomic_inc()
        module_put()
        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
        perf_detach_cgroup() <inline void perf_detach_cgroup (struct perf_event *event) at core.c:828>:
            css_put()
        put_pid_ns()
        kfree()
    IS_ERR()
    PTR_ERR()
    account_event() <void account_event (struct perf_event *event) at core.c:7819>:
        atomic_inc()
        atomic_inc_return()
        tick_nohz_full_kick_all()
        has_branch_stack()
        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
        atomic_inc_not_zero()
        mutex_lock()
        atomic_read()
        static_branch_enable()
        synchronize_sched()
        mutex_unlock()
        account_event_cpu() <void account_event_cpu (struct perf_event *event, int cpu) at core.c:7810>:
            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
            atomic_inc()
            per_cpu()
    find_get_context() <struct perf_event_context *find_get_context (struct pmu *pmu, struct task_struct *task, struct perf_event *event) at core.c:3438>:
        perf_paranoid_cpu()
        capable()
        ERR_PTR()
        cpu_online()
        per_cpu_ptr()
        get_ctx() <void get_ctx (struct perf_event_context *ctx) at core.c:1015>:
            WARN_ON()
            atomic_inc_not_zero()
        kzalloc()
        perf_lock_task_context() <struct perf_event_context *perf_lock_task_context (struct task_struct *task, int ctxn, unsigned long *flags) at core.c:1199>:
            local_irq_save()
            rcu_read_lock()
            rcu_dereference()
            raw_spin_lock()
            raw_spin_unlock()
            rcu_read_unlock()
            local_irq_restore()
            atomic_inc_not_zero()
            WARN_ON_ONCE()
        unclone_ctx()
        raw_spin_unlock_irqrestore()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 514)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
        alloc_perf_context() <struct perf_event_context *alloc_perf_context (struct pmu *pmu, struct task_struct *task) at core.c:3386>:
            kzalloc()
            get_task_struct()
        mutex_lock()
        rcu_assign_pointer()
        mutex_unlock()
        unlikely()
        kfree()
    WARN_ON_ONCE()
    mutex_lock()
    exclusive_event_installable() <bool exclusive_event_installable (struct perf_event *event, struct perf_event_context *ctx) at core.c:3677>:
        list_for_each_entry()
        exclusive_event_match() <bool exclusive_event_match (struct perf_event *e1, struct perf_event *e2) at core.c:3666>:
    perf_install_in_context() <void perf_install_in_context (struct perf_event_context *ctx, struct perf_event *event, int cpu) at core.c:2178>:
        READ_ONCE()
        lockdep_assert_held()
        cpu_function_call() <int cpu_function_call (int cpu, remote_function_f func, void *info) at core.c:126>:
            smp_call_function_single()
            remote_function() <void remote_function (void *data) at core.c:61>:
                task_cpu()
                smp_processor_id()
        WARN_ON_ONCE()
        task_cpu()
        raw_spin_lock_irq()
        raw_spin_unlock_irq()
    perf_unpin_context() <void perf_unpin_context (struct perf_event_context *ctx) at core.c:1268>:
        raw_spin_lock_irqsave()
        raw_spin_unlock_irqrestore()
    mutex_unlock()
    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
        atomic_dec_and_test()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 551)
        put_task_struct()
        call_rcu()
        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
            container_of()
            kfree()
    free_event() <void free_event (struct perf_event *event) at core.c:3740>:
        WARN()
        atomic_long_cmpxchg()
        atomic_long_read()
    ERR_PTR()
perf_event_delayed_put() <void perf_event_delayed_put (struct task_struct *task) at core.c:8999>:
    for_each_task_context_nr()
    WARN_ON_ONCE()
perf_event_disable() <void perf_event_disable (struct perf_event *event) at core.c:1850>:
    perf_event_ctx_lock() <inline struct perf_event_context *perf_event_ctx_lock (struct perf_event *event) at core.c:1125>:
        perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
            rcu_read_lock()
            ACCESS_ONCE()
            atomic_inc_not_zero()
            rcu_read_unlock()
            mutex_lock_nested()
            mutex_unlock()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                atomic_dec_and_test()
                put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 576)
                put_task_struct()
                call_rcu()
                free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                    container_of()
                    kfree()
    perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
        mutex_unlock()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 586)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
perf_event_disable_local() <void perf_event_disable_local (struct perf_event *event) at core.c:1841>:
    event_function_local() <void event_function_local (struct perf_event *event, event_f func, void *data) at core.c:243>:
        event_function() <int event_function (void *info) at core.c:197>:
            WARN_ON_ONCE()
            irqs_disabled()
            perf_ctx_lock() <void perf_ctx_lock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:146>:
                raw_spin_lock()
            perf_ctx_unlock() <void perf_ctx_unlock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:154>:
                raw_spin_unlock()
        WARN_ON_ONCE()
perf_event_enable() <void perf_event_enable (struct perf_event *event) at core.c:2336>:
    perf_event_ctx_lock() <inline struct perf_event_context *perf_event_ctx_lock (struct perf_event *event) at core.c:1125>:
        perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
            rcu_read_lock()
            ACCESS_ONCE()
            atomic_inc_not_zero()
            rcu_read_unlock()
            mutex_lock_nested()
            mutex_unlock()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                atomic_dec_and_test()
                put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 613)
                put_task_struct()
                call_rcu()
                free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                    container_of()
                    kfree()
    perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
        mutex_unlock()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 623)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
perf_event_exec() <void perf_event_exec (void) at core.c:3195>:
    rcu_read_lock()
    for_each_task_context_nr()
    perf_event_enable_on_exec() <void perf_event_enable_on_exec (int ctxn) at core.c:3160>:
        local_irq_save()
        perf_ctx_lock() <void perf_ctx_lock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:146>:
            raw_spin_lock()
        ctx_sched_out() <void ctx_sched_out (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx, enum event_type_t event_type) at core.c:2376>:
            lockdep_assert_held()
            likely()
            WARN_ON_ONCE()
            update_context_time() <void update_context_time (struct perf_event_context *ctx) at core.c:1280>:
                perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                    local_clock()
            update_cgrp_time_from_cpuctx() <inline void update_cgrp_time_from_cpuctx (struct perf_cpu_context *cpuctx) at core.c:845>:
            perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                this_cpu_ptr()
            list_for_each_entry()
            group_sched_out() <void group_sched_out (struct perf_event *group_event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1726>:
                event_sched_out() <void event_sched_out (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1676>:
                    perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                            per_cpu_ptr()
                    WARN_ON_ONCE()
                    lockdep_assert_held()
                    event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                        smp_processor_id()
                        perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                            cgroup_is_descendant()
                        pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                    perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                        this_cpu_ptr()
                    del()
                    is_software_event()
                    perf_event_ctx_deactivate() <void perf_event_ctx_deactivate (struct perf_event_context *ctx) at core.c:1006>:
                        WARN_ON()
                        irqs_disabled()
                        list_empty()
                        list_del_init()
                    perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                        this_cpu_ptr()
                list_for_each_entry()
            perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                this_cpu_ptr()
        list_for_each_entry()
        event_enable_on_exec() <int event_enable_on_exec (struct perf_event *event, struct perf_event_context *ctx) at core.c:3141>:
        unclone_ctx()
        ctx_resched() <void ctx_resched (struct perf_cpu_context *cpuctx, struct perf_event_context *task_ctx) at core.c:2107>:
            perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                this_cpu_ptr()
            task_ctx_sched_out() <void task_ctx_sched_out (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:2083>:
                WARN_ON_ONCE()
                ctx_sched_out() <void ctx_sched_out (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx, enum event_type_t event_type) at core.c:2376>:
                    lockdep_assert_held()
                    likely()
                    WARN_ON_ONCE()
                    update_context_time() <void update_context_time (struct perf_event_context *ctx) at core.c:1280>:
                        perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                            local_clock()
                    update_cgrp_time_from_cpuctx() <inline void update_cgrp_time_from_cpuctx (struct perf_cpu_context *cpuctx) at core.c:845>:
                    perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                        this_cpu_ptr()
                    list_for_each_entry()
                    group_sched_out() <void group_sched_out (struct perf_event *group_event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1726>:
                        event_sched_out() <void event_sched_out (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1676>:
                            perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                    per_cpu_ptr()
                            WARN_ON_ONCE()
                            lockdep_assert_held()
                            event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                                smp_processor_id()
                                perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                    cgroup_is_descendant()
                                pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                            perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                this_cpu_ptr()
                            del()
                            is_software_event()
                            perf_event_ctx_deactivate() <void perf_event_ctx_deactivate (struct perf_event_context *ctx) at core.c:1006>:
                                WARN_ON()
                                irqs_disabled()
                                list_empty()
                                list_del_init()
                            perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                this_cpu_ptr()
                        list_for_each_entry()
                    perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                        this_cpu_ptr()
            cpu_ctx_sched_out() <void cpu_ctx_sched_out (struct perf_cpu_context *cpuctx, enum event_type_t event_type) at core.c:2705>:
                ctx_sched_out() <void ctx_sched_out (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx, enum event_type_t event_type) at core.c:2376>:
                    lockdep_assert_held()
                    likely()
                    WARN_ON_ONCE()
                    update_context_time() <void update_context_time (struct perf_event_context *ctx) at core.c:1280>:
                        perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                            local_clock()
                    update_cgrp_time_from_cpuctx() <inline void update_cgrp_time_from_cpuctx (struct perf_cpu_context *cpuctx) at core.c:845>:
                    perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                        this_cpu_ptr()
                    list_for_each_entry()
                    group_sched_out() <void group_sched_out (struct perf_event *group_event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1726>:
                        event_sched_out() <void event_sched_out (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1676>:
                            perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                    per_cpu_ptr()
                            WARN_ON_ONCE()
                            lockdep_assert_held()
                            event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                                smp_processor_id()
                                perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                    cgroup_is_descendant()
                                pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                            perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                this_cpu_ptr()
                            del()
                            is_software_event()
                            perf_event_ctx_deactivate() <void perf_event_ctx_deactivate (struct perf_event_context *ctx) at core.c:1006>:
                                WARN_ON()
                                irqs_disabled()
                                list_empty()
                                list_del_init()
                            perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                this_cpu_ptr()
                        list_for_each_entry()
                    perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                        this_cpu_ptr()
            perf_event_sched_in() <void perf_event_sched_in (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx, struct task_struct *task) at core.c:2095>:
                cpu_ctx_sched_in() <void cpu_ctx_sched_in (struct perf_cpu_context *cpuctx, enum event_type_t event_type, struct task_struct *task) at core.c:2813>:
                    ctx_sched_in() <void ctx_sched_in (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx, enum event_type_t event_type, struct task_struct *task) at core.c:2771>:
                        lockdep_assert_held()
                        likely()
                        WARN_ON_ONCE()
                        perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                            local_clock()
                        perf_cgroup_set_timestamp() <inline void perf_cgroup_set_timestamp (struct task_struct *task, struct perf_event_context *ctx) at core.c:867>:
                            perf_cgroup_from_task()
                            this_cpu_ptr()
                        ctx_pinned_sched_in() <void ctx_pinned_sched_in (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx) at core.c:2712>:
                            list_for_each_entry()
                            event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                                smp_processor_id()
                                perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                    cgroup_is_descendant()
                                pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                            perf_cgroup_mark_enabled() <inline void perf_cgroup_mark_enabled (struct perf_event *event, struct perf_event_context *ctx) at core.c:893>:
                                perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                        per_cpu_ptr()
                                list_for_each_entry()
                            group_can_go_on() <int group_can_go_on (struct perf_event *event, struct perf_cpu_context *cpuctx, int can_add_hw) at core.c:2034>:
                            group_sched_in() <int group_sched_in (struct perf_event *group_event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1963>:
                                event_sched_in() <int event_sched_in (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1901>:
                                    perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                            per_cpu_ptr()
                                    lockdep_assert_held()
                                    smp_processor_id()
                                    unlikely()
                                    perf_log_throttle() <void perf_log_throttle (struct perf_event *event, int enable) at core.c:6325>:
                                        perf_event_clock() <inline u64 perf_event_clock (struct perf_event *event) at core.c:510>
                                        primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
                                        perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                                        perf_output_begin()
                                        perf_output_put()
                                        perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                                        perf_output_end()
                                    smp_wmb()
                                    perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                        this_cpu_ptr()
                                    perf_set_shadow_time() <void perf_set_shadow_time (struct perf_event *event, struct perf_event_context *ctx, u64 tstamp) at core.c:1860>:
                                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                        perf_cgroup_set_shadow_time() <inline void perf_cgroup_set_shadow_time (struct perf_event *event, u64 now) at core.c:878>:
                                            per_cpu_ptr()
                                    perf_log_itrace_start() <void perf_log_itrace_start (struct perf_event *event) at core.c:6362>:
                                        perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                                            task_tgid_nr_ns()
                                        perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                                            task_pid_nr_ns()
                                        perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                                        perf_output_begin()
                                        perf_output_put()
                                        perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                                        perf_output_end()
                                    add()
                                    is_software_event()
                                    perf_event_ctx_activate() <void perf_event_ctx_activate (struct perf_event_context *ctx) at core.c:995>:
                                        this_cpu_ptr()
                                        WARN_ON()
                                        irqs_disabled()
                                        list_empty()
                                        list_add()
                                    perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                        this_cpu_ptr()
                                perf_mux_hrtimer_restart() <int perf_mux_hrtimer_restart (struct perf_cpu_context *cpuctx) at core.c:952>:
                                    raw_spin_lock_irqsave()
                                    hrtimer_forward_now()
                                    hrtimer_start_expires()
                                    raw_spin_unlock_irqrestore()
                                list_for_each_entry()
                                event_sched_out() <void event_sched_out (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1676>:
                                    perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                            per_cpu_ptr()
                                    WARN_ON_ONCE()
                                    lockdep_assert_held()
                                    event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                                        smp_processor_id()
                                        perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                            cgroup_is_descendant()
                                        pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                                    perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                        this_cpu_ptr()
                                    del()
                                    is_software_event()
                                    perf_event_ctx_deactivate() <void perf_event_ctx_deactivate (struct perf_event_context *ctx) at core.c:1006>:
                                        WARN_ON()
                                        irqs_disabled()
                                        list_empty()
                                        list_del_init()
                                    perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                        this_cpu_ptr()
                            update_group_times() <void update_group_times (struct perf_event *leader) at core.c:1343>:
                                update_event_times() <void update_event_times (struct perf_event *event) at core.c:1301>:
                                    lockdep_assert_held()
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                        per_cpu_ptr()
                                    perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                            per_cpu_ptr()
                                list_for_each_entry()
                        ctx_flexible_sched_in() <void ctx_flexible_sched_in (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx) at core.c:2742>:
                            list_for_each_entry()
                            event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                                smp_processor_id()
                                perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                    cgroup_is_descendant()
                                pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                            perf_cgroup_mark_enabled() <inline void perf_cgroup_mark_enabled (struct perf_event *event, struct perf_event_context *ctx) at core.c:893>:
                                perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                        per_cpu_ptr()
                                list_for_each_entry()
                            group_can_go_on() <int group_can_go_on (struct perf_event *event, struct perf_cpu_context *cpuctx, int can_add_hw) at core.c:2034>:
                            group_sched_in() <int group_sched_in (struct perf_event *group_event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1963>:
                                event_sched_in() <int event_sched_in (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1901>:
                                    perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                            per_cpu_ptr()
                                    lockdep_assert_held()
                                    smp_processor_id()
                                    unlikely()
                                    perf_log_throttle() <void perf_log_throttle (struct perf_event *event, int enable) at core.c:6325>:
                                        perf_event_clock() <inline u64 perf_event_clock (struct perf_event *event) at core.c:510>
                                        primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
                                        perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                                        perf_output_begin()
                                        perf_output_put()
                                        perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                                        perf_output_end()
                                    smp_wmb()
                                    perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                        this_cpu_ptr()
                                    perf_set_shadow_time() <void perf_set_shadow_time (struct perf_event *event, struct perf_event_context *ctx, u64 tstamp) at core.c:1860>:
                                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                        perf_cgroup_set_shadow_time() <inline void perf_cgroup_set_shadow_time (struct perf_event *event, u64 now) at core.c:878>:
                                            per_cpu_ptr()
                                    perf_log_itrace_start() <void perf_log_itrace_start (struct perf_event *event) at core.c:6362>:
                                        perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                                            task_tgid_nr_ns()
                                        perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                                            task_pid_nr_ns()
                                        perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                                        perf_output_begin()
                                        perf_output_put()
                                        perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                                        perf_output_end()
                                    add()
                                    is_software_event()
                                    perf_event_ctx_activate() <void perf_event_ctx_activate (struct perf_event_context *ctx) at core.c:995>:
                                        this_cpu_ptr()
                                        WARN_ON()
                                        irqs_disabled()
                                        list_empty()
                                        list_add()
                                    perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                        this_cpu_ptr()
                                perf_mux_hrtimer_restart() <int perf_mux_hrtimer_restart (struct perf_cpu_context *cpuctx) at core.c:952>:
                                    raw_spin_lock_irqsave()
                                    hrtimer_forward_now()
                                    hrtimer_start_expires()
                                    raw_spin_unlock_irqrestore()
                                list_for_each_entry()
                                event_sched_out() <void event_sched_out (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1676>:
                                    perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                            per_cpu_ptr()
                                    WARN_ON_ONCE()
                                    lockdep_assert_held()
                                    event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                                        smp_processor_id()
                                        perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                            cgroup_is_descendant()
                                        pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                                    perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                        this_cpu_ptr()
                                    del()
                                    is_software_event()
                                    perf_event_ctx_deactivate() <void perf_event_ctx_deactivate (struct perf_event_context *ctx) at core.c:1006>:
                                        WARN_ON()
                                        irqs_disabled()
                                        list_empty()
                                        list_del_init()
                                    perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                        this_cpu_ptr()
                ctx_sched_in() <void ctx_sched_in (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx, enum event_type_t event_type, struct task_struct *task) at core.c:2771>:
                    lockdep_assert_held()
                    likely()
                    WARN_ON_ONCE()
                    perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                        local_clock()
                    perf_cgroup_set_timestamp() <inline void perf_cgroup_set_timestamp (struct task_struct *task, struct perf_event_context *ctx) at core.c:867>:
                        perf_cgroup_from_task()
                        this_cpu_ptr()
                    ctx_pinned_sched_in() <void ctx_pinned_sched_in (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx) at core.c:2712>:
                        list_for_each_entry()
                        event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                            smp_processor_id()
                            perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                cgroup_is_descendant()
                            pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                        perf_cgroup_mark_enabled() <inline void perf_cgroup_mark_enabled (struct perf_event *event, struct perf_event_context *ctx) at core.c:893>:
                            perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                    per_cpu_ptr()
                            list_for_each_entry()
                        group_can_go_on() <int group_can_go_on (struct perf_event *event, struct perf_cpu_context *cpuctx, int can_add_hw) at core.c:2034>:
                        group_sched_in() <int group_sched_in (struct perf_event *group_event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1963>:
                            event_sched_in() <int event_sched_in (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1901>:
                                perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                        per_cpu_ptr()
                                lockdep_assert_held()
                                smp_processor_id()
                                unlikely()
                                perf_log_throttle() <void perf_log_throttle (struct perf_event *event, int enable) at core.c:6325>:
                                    perf_event_clock() <inline u64 perf_event_clock (struct perf_event *event) at core.c:510>
                                    primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
                                    perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                                    perf_output_begin()
                                    perf_output_put()
                                    perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                                    perf_output_end()
                                smp_wmb()
                                perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                    this_cpu_ptr()
                                perf_set_shadow_time() <void perf_set_shadow_time (struct perf_event *event, struct perf_event_context *ctx, u64 tstamp) at core.c:1860>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_set_shadow_time() <inline void perf_cgroup_set_shadow_time (struct perf_event *event, u64 now) at core.c:878>:
                                        per_cpu_ptr()
                                perf_log_itrace_start() <void perf_log_itrace_start (struct perf_event *event) at core.c:6362>:
                                    perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                                        task_tgid_nr_ns()
                                    perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                                        task_pid_nr_ns()
                                    perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                                    perf_output_begin()
                                    perf_output_put()
                                    perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                                    perf_output_end()
                                add()
                                is_software_event()
                                perf_event_ctx_activate() <void perf_event_ctx_activate (struct perf_event_context *ctx) at core.c:995>:
                                    this_cpu_ptr()
                                    WARN_ON()
                                    irqs_disabled()
                                    list_empty()
                                    list_add()
                                perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                    this_cpu_ptr()
                            perf_mux_hrtimer_restart() <int perf_mux_hrtimer_restart (struct perf_cpu_context *cpuctx) at core.c:952>:
                                raw_spin_lock_irqsave()
                                hrtimer_forward_now()
                                hrtimer_start_expires()
                                raw_spin_unlock_irqrestore()
                            list_for_each_entry()
                            event_sched_out() <void event_sched_out (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1676>:
                                perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                        per_cpu_ptr()
                                WARN_ON_ONCE()
                                lockdep_assert_held()
                                event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                                    smp_processor_id()
                                    perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                        cgroup_is_descendant()
                                    pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                                perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                    this_cpu_ptr()
                                del()
                                is_software_event()
                                perf_event_ctx_deactivate() <void perf_event_ctx_deactivate (struct perf_event_context *ctx) at core.c:1006>:
                                    WARN_ON()
                                    irqs_disabled()
                                    list_empty()
                                    list_del_init()
                                perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                    this_cpu_ptr()
                        update_group_times() <void update_group_times (struct perf_event *leader) at core.c:1343>:
                            update_event_times() <void update_event_times (struct perf_event *event) at core.c:1301>:
                                lockdep_assert_held()
                                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                    per_cpu_ptr()
                                perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                        per_cpu_ptr()
                            list_for_each_entry()
                    ctx_flexible_sched_in() <void ctx_flexible_sched_in (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx) at core.c:2742>:
                        list_for_each_entry()
                        event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                            smp_processor_id()
                            perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                cgroup_is_descendant()
                            pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                        perf_cgroup_mark_enabled() <inline void perf_cgroup_mark_enabled (struct perf_event *event, struct perf_event_context *ctx) at core.c:893>:
                            perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                    per_cpu_ptr()
                            list_for_each_entry()
                        group_can_go_on() <int group_can_go_on (struct perf_event *event, struct perf_cpu_context *cpuctx, int can_add_hw) at core.c:2034>:
                        group_sched_in() <int group_sched_in (struct perf_event *group_event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1963>:
                            event_sched_in() <int event_sched_in (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1901>:
                                perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                        per_cpu_ptr()
                                lockdep_assert_held()
                                smp_processor_id()
                                unlikely()
                                perf_log_throttle() <void perf_log_throttle (struct perf_event *event, int enable) at core.c:6325>:
                                    perf_event_clock() <inline u64 perf_event_clock (struct perf_event *event) at core.c:510>
                                    primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
                                    perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                                    perf_output_begin()
                                    perf_output_put()
                                    perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                                    perf_output_end()
                                smp_wmb()
                                perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                    this_cpu_ptr()
                                perf_set_shadow_time() <void perf_set_shadow_time (struct perf_event *event, struct perf_event_context *ctx, u64 tstamp) at core.c:1860>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_set_shadow_time() <inline void perf_cgroup_set_shadow_time (struct perf_event *event, u64 now) at core.c:878>:
                                        per_cpu_ptr()
                                perf_log_itrace_start() <void perf_log_itrace_start (struct perf_event *event) at core.c:6362>:
                                    perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                                        task_tgid_nr_ns()
                                    perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                                        task_pid_nr_ns()
                                    perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                                    perf_output_begin()
                                    perf_output_put()
                                    perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                                    perf_output_end()
                                add()
                                is_software_event()
                                perf_event_ctx_activate() <void perf_event_ctx_activate (struct perf_event_context *ctx) at core.c:995>:
                                    this_cpu_ptr()
                                    WARN_ON()
                                    irqs_disabled()
                                    list_empty()
                                    list_add()
                                perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                    this_cpu_ptr()
                            perf_mux_hrtimer_restart() <int perf_mux_hrtimer_restart (struct perf_cpu_context *cpuctx) at core.c:952>:
                                raw_spin_lock_irqsave()
                                hrtimer_forward_now()
                                hrtimer_start_expires()
                                raw_spin_unlock_irqrestore()
                            list_for_each_entry()
                            event_sched_out() <void event_sched_out (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1676>:
                                perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                        per_cpu_ptr()
                                WARN_ON_ONCE()
                                lockdep_assert_held()
                                event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                                    smp_processor_id()
                                    perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                        cgroup_is_descendant()
                                    pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                                perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                                    this_cpu_ptr()
                                del()
                                is_software_event()
                                perf_event_ctx_deactivate() <void perf_event_ctx_deactivate (struct perf_event_context *ctx) at core.c:1006>:
                                    WARN_ON()
                                    irqs_disabled()
                                    list_empty()
                                    list_del_init()
                                perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                                    this_cpu_ptr()
            perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                this_cpu_ptr()
        perf_ctx_unlock() <void perf_ctx_unlock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:154>:
            raw_spin_unlock()
        local_irq_restore()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 1160)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
    rcu_read_unlock()
perf_event_exit_task() <void perf_event_exit_task (struct task_struct *child) at core.c:8909>:
    mutex_lock()
    list_for_each_entry_safe()
    list_del_init()
    smp_store_release()
    mutex_unlock()
    for_each_task_context_nr()
    perf_event_exit_task_context() <void perf_event_exit_task_context (struct task_struct *child, int ctxn) at core.c:8845>:
        WARN_ON_ONCE()
        perf_pin_task_context() <struct perf_event_context *perf_pin_task_context (struct task_struct *task, int ctxn) at core.c:1255>:
            perf_lock_task_context() <struct perf_event_context *perf_lock_task_context (struct task_struct *task, int ctxn, unsigned long *flags) at core.c:1199>:
                local_irq_save()
                rcu_read_lock()
                rcu_dereference()
                raw_spin_lock()
                raw_spin_unlock()
                rcu_read_unlock()
                local_irq_restore()
                atomic_inc_not_zero()
                WARN_ON_ONCE()
            raw_spin_unlock_irqrestore()
        mutex_lock()
        raw_spin_lock_irq()
        task_ctx_sched_out() <void task_ctx_sched_out (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:2083>:
            WARN_ON_ONCE()
            ctx_sched_out() <void ctx_sched_out (struct perf_event_context *ctx, struct perf_cpu_context *cpuctx, enum event_type_t event_type) at core.c:2376>:
                lockdep_assert_held()
                likely()
                WARN_ON_ONCE()
                update_context_time() <void update_context_time (struct perf_event_context *ctx) at core.c:1280>:
                    perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                        local_clock()
                update_cgrp_time_from_cpuctx() <inline void update_cgrp_time_from_cpuctx (struct perf_cpu_context *cpuctx) at core.c:845>:
                perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                    this_cpu_ptr()
                list_for_each_entry()
                group_sched_out() <void group_sched_out (struct perf_event *group_event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1726>:
                    event_sched_out() <void event_sched_out (struct perf_event *event, struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:1676>:
                        perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                            perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                per_cpu_ptr()
                        WARN_ON_ONCE()
                        lockdep_assert_held()
                        event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                            smp_processor_id()
                            perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                cgroup_is_descendant()
                            pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                        perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                            this_cpu_ptr()
                        del()
                        is_software_event()
                        perf_event_ctx_deactivate() <void perf_event_ctx_deactivate (struct perf_event_context *ctx) at core.c:1006>:
                            WARN_ON()
                            irqs_disabled()
                            list_empty()
                            list_del_init()
                        perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                            this_cpu_ptr()
                    list_for_each_entry()
                perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                    this_cpu_ptr()
        RCU_INIT_POINTER()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 1233)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
        WRITE_ONCE()
        put_task_struct()
        unclone_ctx()
        raw_spin_unlock_irq()
        perf_event_task() <void perf_event_task (struct task_struct *task, struct perf_event_context *task_ctx, int new) at core.c:5790>:
            atomic_read()
            perf_event_aux() <void perf_event_aux (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5689>:
                perf_event_aux_task_ctx() <void perf_event_aux_task_ctx (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5678>:
                    rcu_read_lock()
                    preempt_disable()
                    perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                        list_for_each_entry_rcu()
                        event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                            smp_processor_id()
                            perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                                cgroup_is_descendant()
                            pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                    preempt_enable()
                    rcu_read_unlock()
                rcu_read_lock()
                list_for_each_entry_rcu()
                get_cpu_ptr()
                perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                    list_for_each_entry_rcu()
                    event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                        smp_processor_id()
                        perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                            cgroup_is_descendant()
                        pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                rcu_dereference()
                put_cpu_ptr()
                rcu_read_unlock()
            perf_event_task_output() <void perf_event_task_output (struct perf_event *event, void *data) at core.c:5754>:
                perf_event_task_match() <int perf_event_task_match (struct perf_event *event) at core.c:5747>:
                perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                perf_output_begin()
                perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                    task_tgid_nr_ns()
                perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                    task_pid_nr_ns()
                perf_event_clock() <inline u64 perf_event_clock (struct perf_event *event) at core.c:510>
                perf_output_put()
                perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                perf_output_end()
        list_for_each_entry_safe()
        perf_event_exit_event() <void perf_event_exit_event (struct perf_event *child_event, struct perf_event_context *child_ctx, struct task_struct *child) at core.c:8789>:
            raw_spin_lock_irq()
            WARN_ON_ONCE()
            perf_group_detach() <void perf_group_detach (struct perf_event *event) at core.c:1609>:
                list_del_init()
                list_empty()
                list_for_each_entry_safe()
                list_move_tail()
                WARN_ON_ONCE()
                perf_event__header_size() <void perf_event__header_size (struct perf_event *event) at core.c:1465>:
                list_for_each_entry()
            list_del_event() <void list_del_event (struct perf_event *event, struct perf_event_context *ctx) at core.c:1555>:
                WARN_ON_ONCE()
                lockdep_assert_held()
                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                list_del_rcu()
                list_del_init()
                update_group_times() <void update_group_times (struct perf_event *leader) at core.c:1343>:
                    update_event_times() <void update_event_times (struct perf_event *event) at core.c:1301>:
                        lockdep_assert_held()
                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                            per_cpu_ptr()
                        perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                            perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                per_cpu_ptr()
                    list_for_each_entry()
            raw_spin_unlock_irq()
            perf_event_wakeup() <void perf_event_wakeup (struct perf_event *event) at core.c:4976>:
                ring_buffer_wakeup() <void ring_buffer_wakeup (struct perf_event *event) at core.c:4573>:
                    rcu_read_lock()
                    rcu_dereference()
                    list_for_each_entry_rcu()
                    wake_up_all()
                    rcu_read_unlock()
                kill_fasync()
                perf_event_fasync() <inline struct fasync_struct **perf_event_fasync (struct perf_event *event) at core.c:4968>
            sync_child_event() <void sync_child_event (struct perf_event *child_event, struct task_struct *child) at core.c:8767>:
                perf_event_read_event() <void perf_event_read_event (struct perf_event *event, struct task_struct *task) at core.c:5631>:
                    perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                        task_tgid_nr_ns()
                    perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                        task_pid_nr_ns()
                    perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
                    perf_output_begin()
                    perf_output_put()
                    perf_output_read() <void perf_output_read (struct perf_output_handle *handle, struct perf_event *event) at core.c:5306>:
                        calc_timer_values() <void calc_timer_values (struct perf_event *event, u64 *now, u64 *enabled, u64 *running) at core.c:4397>:
                            perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                                local_clock()
                        perf_output_read_group() <void perf_output_read_group (struct perf_output_handle *handle, struct perf_event *event, u64 enabled, u64 running) at core.c:5262>:
                            perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
                                count()
                            primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
                            list_for_each_entry()
                        perf_output_read_one() <void perf_output_read_one (struct perf_output_handle *handle, struct perf_event *event, u64 enabled, u64 running) at core.c:5236>:
                            perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
                                count()
                            atomic64_read()
                            primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
                    perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
                    perf_output_end()
                perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
                    count()
                atomic64_add()
            mutex_lock()
            list_del_init()
            mutex_unlock()
            free_event() <void free_event (struct perf_event *event) at core.c:3740>:
                WARN()
                atomic_long_cmpxchg()
                atomic_long_read()
            put_event() <void put_event (struct perf_event *event) at core.c:3803>:
                atomic_long_dec_and_test()
        mutex_unlock()
    perf_event_task() <void perf_event_task (struct task_struct *task, struct perf_event_context *task_ctx, int new) at core.c:5790>:
        atomic_read()
        perf_event_aux() <void perf_event_aux (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5689>:
            perf_event_aux_task_ctx() <void perf_event_aux_task_ctx (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5678>:
                rcu_read_lock()
                preempt_disable()
                perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                    list_for_each_entry_rcu()
                    event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                        smp_processor_id()
                        perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                            cgroup_is_descendant()
                        pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                preempt_enable()
                rcu_read_unlock()
            rcu_read_lock()
            list_for_each_entry_rcu()
            get_cpu_ptr()
            perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                list_for_each_entry_rcu()
                event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                    smp_processor_id()
                    perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                        cgroup_is_descendant()
                    pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
            rcu_dereference()
            put_cpu_ptr()
            rcu_read_unlock()
        perf_event_task_output() <void perf_event_task_output (struct perf_event *event, void *data) at core.c:5754>:
            perf_event_task_match() <int perf_event_task_match (struct perf_event *event) at core.c:5747>:
            perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
            perf_output_begin()
            perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                task_tgid_nr_ns()
            perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                task_pid_nr_ns()
            perf_event_clock() <inline u64 perf_event_clock (struct perf_event *event) at core.c:510>
            perf_output_put()
            perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
            perf_output_end()
perf_event_fork() <void perf_event_fork (struct task_struct *task) at core.c:5823>:
    perf_event_task() <void perf_event_task (struct task_struct *task, struct perf_event_context *task_ctx, int new) at core.c:5790>:
        atomic_read()
        perf_event_aux() <void perf_event_aux (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5689>:
            perf_event_aux_task_ctx() <void perf_event_aux_task_ctx (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5678>:
                rcu_read_lock()
                preempt_disable()
                perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                    list_for_each_entry_rcu()
                    event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                        smp_processor_id()
                        perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                            cgroup_is_descendant()
                        pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                preempt_enable()
                rcu_read_unlock()
            rcu_read_lock()
            list_for_each_entry_rcu()
            get_cpu_ptr()
            perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                list_for_each_entry_rcu()
                event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                    smp_processor_id()
                    perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                        cgroup_is_descendant()
                    pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
            rcu_dereference()
            put_cpu_ptr()
            rcu_read_unlock()
        perf_event_task_output() <void perf_event_task_output (struct perf_event *event, void *data) at core.c:5754>:
            perf_event_task_match() <int perf_event_task_match (struct perf_event *event) at core.c:5747>:
            perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
            perf_output_begin()
            perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                task_tgid_nr_ns()
            perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                task_pid_nr_ns()
            perf_event_clock() <inline u64 perf_event_clock (struct perf_event *event) at core.c:510>
            perf_output_put()
            perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
            perf_output_end()
perf_event_free_task() <void perf_event_free_task (struct task_struct *task) at core.c:8968>:
    for_each_task_context_nr()
    mutex_lock()
    list_for_each_entry_safe()
    perf_free_event() <void perf_free_event (struct perf_event *event, struct perf_event_context *ctx) at core.c:8940>:
        WARN_ON_ONCE()
        mutex_lock()
        list_del_init()
        mutex_unlock()
        put_event() <void put_event (struct perf_event *event) at core.c:3803>:
            atomic_long_dec_and_test()
        raw_spin_lock_irq()
        perf_group_detach() <void perf_group_detach (struct perf_event *event) at core.c:1609>:
            list_del_init()
            list_empty()
            list_for_each_entry_safe()
            list_move_tail()
            WARN_ON_ONCE()
            perf_event__header_size() <void perf_event__header_size (struct perf_event *event) at core.c:1465>:
            list_for_each_entry()
        list_del_event() <void list_del_event (struct perf_event *event, struct perf_event_context *ctx) at core.c:1555>:
            WARN_ON_ONCE()
            lockdep_assert_held()
            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
            list_del_rcu()
            list_del_init()
            update_group_times() <void update_group_times (struct perf_event *leader) at core.c:1343>:
                update_event_times() <void update_event_times (struct perf_event *event) at core.c:1301>:
                    lockdep_assert_held()
                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                        per_cpu_ptr()
                    perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                            per_cpu_ptr()
                list_for_each_entry()
        raw_spin_unlock_irq()
        free_event() <void free_event (struct perf_event *event) at core.c:3740>:
            WARN()
            atomic_long_cmpxchg()
            atomic_long_read()
    list_empty()
    mutex_unlock()
    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
        atomic_dec_and_test()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 1488)
        put_task_struct()
        call_rcu()
        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
            container_of()
            kfree()
perf_event_get() <struct file *perf_event_get (unsigned int fd) at core.c:9007>:
    fget_raw()
    ERR_PTR()
    fput()
perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
perf_event_init() <void __init perf_event_init (void) at core.c:9413>:
    idr_init()
    perf_event_init_all_cpus() <void __init perf_event_init_all_cpus (void) at core.c:9307>:
        for_each_possible_cpu()
        per_cpu()
        mutex_init()
        INIT_LIST_HEAD()
    init_srcu_struct()
    perf_pmu_register() <int perf_pmu_register (struct pmu *pmu, const char *name, int type) at core.c:7621>:
        mutex_lock()
        alloc_percpu()
        idr_alloc()
        pmu_dev_alloc() <int pmu_dev_alloc (struct pmu *pmu) at core.c:7589>:
            kzalloc()
            device_initialize()
            dev_set_name()
            dev_set_drvdata()
            pmu_dev_release() <void pmu_dev_release (struct device *dev) at core.c:7584>:
                kfree()
            device_add()
            put_device()
        find_pmu_context() <struct perf_cpu_context __percpu *find_pmu_context (int ctxn) at core.c:7461>:
            list_for_each_entry()
        for_each_possible_cpu()
        per_cpu_ptr()
        lockdep_set_class()
        perf_pmu_start_txn() <void perf_pmu_start_txn (struct pmu *pmu, unsigned int flags) at core.c:7417>:
            perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                this_cpu_ptr()
        perf_pmu_commit_txn() <int perf_pmu_commit_txn (struct pmu *pmu) at core.c:7427>:
            perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                this_cpu_ptr()
        perf_pmu_cancel_txn() <void perf_pmu_cancel_txn (struct pmu *pmu) at core.c:7440>:
            perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                this_cpu_ptr()
        perf_pmu_nop_txn() <void perf_pmu_nop_txn (struct pmu *pmu, unsigned int flags) at core.c:7406>
        perf_pmu_nop_int() <int perf_pmu_nop_int (struct pmu *pmu) at core.c:7410>
        perf_pmu_nop_void() <void perf_pmu_nop_void (struct pmu *pmu) at core.c:7402>
        perf_event_idx_default() <int perf_event_idx_default (struct perf_event *event) at core.c:7452>
        list_add_rcu()
        atomic_set()
        mutex_unlock()
        device_del()
        put_device()
        idr_remove()
        free_percpu()
    perf_tp_register() <inline void perf_tp_register (void) at core.c:7116>:
        perf_pmu_register() <int perf_pmu_register (struct pmu *pmu, const char *name, int type) at core.c:7621>:
            mutex_lock()
            alloc_percpu()
            idr_alloc()
            pmu_dev_alloc() <int pmu_dev_alloc (struct pmu *pmu) at core.c:7589>:
                kzalloc()
                device_initialize()
                dev_set_name()
                dev_set_drvdata()
                pmu_dev_release() <void pmu_dev_release (struct device *dev) at core.c:7584>:
                    kfree()
                device_add()
                put_device()
            find_pmu_context() <struct perf_cpu_context __percpu *find_pmu_context (int ctxn) at core.c:7461>:
                list_for_each_entry()
            for_each_possible_cpu()
            per_cpu_ptr()
            lockdep_set_class()
            perf_pmu_start_txn() <void perf_pmu_start_txn (struct pmu *pmu, unsigned int flags) at core.c:7417>:
                perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
                    this_cpu_ptr()
            perf_pmu_commit_txn() <int perf_pmu_commit_txn (struct pmu *pmu) at core.c:7427>:
                perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                    this_cpu_ptr()
            perf_pmu_cancel_txn() <void perf_pmu_cancel_txn (struct pmu *pmu) at core.c:7440>:
                perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
                    this_cpu_ptr()
            perf_pmu_nop_txn() <void perf_pmu_nop_txn (struct pmu *pmu, unsigned int flags) at core.c:7406>
            perf_pmu_nop_int() <int perf_pmu_nop_int (struct pmu *pmu) at core.c:7410>
            perf_pmu_nop_void() <void perf_pmu_nop_void (struct pmu *pmu) at core.c:7402>
            perf_event_idx_default() <int perf_event_idx_default (struct perf_event *event) at core.c:7452>
            list_add_rcu()
            atomic_set()
            mutex_unlock()
            device_del()
            put_device()
            idr_remove()
            free_percpu()
    perf_cpu_notifier()
    perf_cpu_notify() <int perf_cpu_notify (struct notifier_block *self, unsigned long action, void *hcpu) at core.c:9393>:
        perf_event_init_cpu() <void perf_event_init_cpu (int cpu) at core.c:9319>:
            per_cpu()
            mutex_lock()
            swevent_hlist_deref() <inline struct swevent_hlist *swevent_hlist_deref (struct swevent_htable *swhash) at core.c:6771>:
                rcu_dereference_protected()
                lockdep_is_held()
            kzalloc_node()
            cpu_to_node()
            WARN_ON()
            rcu_assign_pointer()
            mutex_unlock()
        perf_event_exit_cpu() <inline void perf_event_exit_cpu (int cpu) at core.c:9369>:
            perf_event_exit_cpu_context() <void perf_event_exit_cpu_context (int cpu) at core.c:9347>:
                srcu_read_lock()
                list_for_each_entry_rcu()
                per_cpu_ptr()
                mutex_lock()
                smp_call_function_single()
                mutex_unlock()
                srcu_read_unlock()
    register_reboot_notifier()
    init_hw_breakpoint()
    WARN()
    BUILD_BUG_ON()
perf_event_init_task() <int perf_event_init_task (struct task_struct *child) at core.c:9288>:
    mutex_init()
    INIT_LIST_HEAD()
    for_each_task_context_nr()
    perf_event_init_context() <int perf_event_init_context (struct task_struct *child, int ctxn) at core.c:9191>:
        likely()
        perf_pin_task_context() <struct perf_event_context *perf_pin_task_context (struct task_struct *task, int ctxn) at core.c:1255>:
            perf_lock_task_context() <struct perf_event_context *perf_lock_task_context (struct task_struct *task, int ctxn, unsigned long *flags) at core.c:1199>:
                local_irq_save()
                rcu_read_lock()
                rcu_dereference()
                raw_spin_lock()
                raw_spin_unlock()
                rcu_read_unlock()
                local_irq_restore()
                atomic_inc_not_zero()
                WARN_ON_ONCE()
            raw_spin_unlock_irqrestore()
        mutex_lock()
        list_for_each_entry()
        inherit_task_group() <int inherit_task_group (struct perf_event *event, struct task_struct *parent, struct perf_event_context *parent_ctx, struct task_struct *child, int ctxn, int *inherited_all) at core.c:9150>:
            alloc_perf_context() <struct perf_event_context *alloc_perf_context (struct pmu *pmu, struct task_struct *task) at core.c:3386>:
                kzalloc()
                get_task_struct()
            inherit_group() <int inherit_group (struct perf_event *parent_event, struct task_struct *parent, struct perf_event_context *parent_ctx, struct task_struct *child, struct perf_event_context *child_ctx) at core.c:9126>:
                inherit_event() <struct perf_event *inherit_event (struct perf_event *parent_event, struct task_struct *parent, struct perf_event_context *parent_ctx, struct task_struct *child, struct perf_event *group_leader, struct perf_event_context *child_ctx) at core.c:9035>:
                    perf_event_alloc() <struct perf_event *perf_event_alloc (struct perf_event_attr *attr, int cpu, struct task_struct *task, struct perf_event *group_leader, struct perf_event *parent_event, perf_overflow_handler_t overflow_handler, void *context, int cgroup_fd) at core.c:7877>:
                        ERR_PTR()
                        kzalloc()
                        mutex_init()
                        INIT_LIST_HEAD()
                        INIT_HLIST_NODE()
                        init_waitqueue_head()
                        init_irq_work()
                        perf_pending_event() <void perf_pending_event (struct irq_work *entry) at core.c:4986>:
                            container_of()
                            perf_swevent_get_recursion_context() <int perf_swevent_get_recursion_context (void) at core.c:6685>:
                                this_cpu_ptr()
                                get_recursion_context()
                            perf_event_disable_local() <void perf_event_disable_local (struct perf_event *event) at core.c:1841>:
                                event_function_local() <void event_function_local (struct perf_event *event, event_f func, void *data) at core.c:243>:
                                    event_function() <int event_function (void *info) at core.c:197>:
                                        WARN_ON_ONCE()
                                        irqs_disabled()
                                        perf_ctx_lock() <void perf_ctx_lock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:146>:
                                            raw_spin_lock()
                                        perf_ctx_unlock() <void perf_ctx_unlock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:154>:
                                            raw_spin_unlock()
                                    WARN_ON_ONCE()
                            perf_event_wakeup() <void perf_event_wakeup (struct perf_event *event) at core.c:4976>:
                                ring_buffer_wakeup() <void ring_buffer_wakeup (struct perf_event *event) at core.c:4573>:
                                    rcu_read_lock()
                                    rcu_dereference()
                                    list_for_each_entry_rcu()
                                    wake_up_all()
                                    rcu_read_unlock()
                                kill_fasync()
                                perf_event_fasync() <inline struct fasync_struct **perf_event_fasync (struct perf_event *event) at core.c:4968>
                            perf_swevent_put_recursion_context() <inline void perf_swevent_put_recursion_context (int rctx) at core.c:6693>:
                                this_cpu_ptr()
                                put_recursion_context()
                        atomic_long_set()
                        get_pid_ns()
                        task_active_pid_ns()
                        atomic64_inc_return()
                        local_clock()
                        perf_event__state_init() <inline void perf_event__state_init (struct perf_event *event) at core.c:1402>:
                        local64_set()
                        has_branch_stack()
                        perf_cgroup_connect() <inline int perf_cgroup_connect (pid_t pid, struct perf_event *event, struct perf_event_attr *attr, struct perf_event *group_leader) at core.c:859>:
                            fdget()
                            css_tryget_online_from_dir()
                            IS_ERR()
                            PTR_ERR()
                            container_of()
                            perf_detach_cgroup() <inline void perf_detach_cgroup (struct perf_event *event) at core.c:828>:
                                css_put()
                            fdput()
                        perf_init_event() <struct pmu *perf_init_event (struct perf_event *event) at core.c:7775>:
                            srcu_read_lock()
                            rcu_read_lock()
                            idr_find()
                            rcu_read_unlock()
                            perf_try_init_event() <int perf_try_init_event (struct pmu *pmu, struct perf_event *event) at core.c:7745>:
                                try_module_get()
                                perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
                                    rcu_read_lock()
                                    ACCESS_ONCE()
                                    atomic_inc_not_zero()
                                    rcu_read_unlock()
                                    mutex_lock_nested()
                                    mutex_unlock()
                                    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                                        atomic_dec_and_test()
                                        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 1706)
                                        put_task_struct()
                                        call_rcu()
                                        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                                            container_of()
                                            kfree()
                                BUG_ON()
                                perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
                                    mutex_unlock()
                                    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                                        atomic_dec_and_test()
                                        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 1717)
                                        put_task_struct()
                                        call_rcu()
                                        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                                            container_of()
                                            kfree()
                                module_put()
                            ERR_PTR()
                            list_for_each_entry_rcu()
                            srcu_read_unlock()
                        IS_ERR()
                        PTR_ERR()
                        exclusive_event_init() <int exclusive_event_init (struct perf_event *event) at core.c:3621>:
                            atomic_inc_unless_negative()
                            atomic_dec_unless_positive()
                        get_callchain_buffers()
                        exclusive_event_destroy() <void exclusive_event_destroy (struct perf_event *event) at core.c:3652>:
                            atomic_dec()
                            atomic_inc()
                        module_put()
                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                        perf_detach_cgroup() <inline void perf_detach_cgroup (struct perf_event *event) at core.c:828>:
                            css_put()
                        put_pid_ns()
                        kfree()
                    IS_ERR()
                    mutex_lock()
                    is_orphaned_event() <bool is_orphaned_event (struct perf_event *event) at core.c:1657>:
                    atomic_long_inc_not_zero()
                    mutex_unlock()
                    free_event() <void free_event (struct perf_event *event) at core.c:3740>:
                        WARN()
                        atomic_long_cmpxchg()
                        atomic_long_read()
                    get_ctx() <void get_ctx (struct perf_event_context *ctx) at core.c:1015>:
                        WARN_ON()
                        atomic_inc_not_zero()
                    local64_set()
                    perf_event__header_size() <void perf_event__header_size (struct perf_event *event) at core.c:1465>:
                    perf_event__id_header_size() <void perf_event__id_header_size (struct perf_event *event) at core.c:1472>:
                    raw_spin_lock_irqsave()
                    add_event_to_ctx() <void add_event_to_ctx (struct perf_event *event, struct perf_event_context *ctx) at core.c:2062>:
                        perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                            perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                per_cpu_ptr()
                        list_add_event() <void list_add_event (struct perf_event *event, struct perf_event_context *ctx) at core.c:1366>:
                            lockdep_assert_held()
                            WARN_ON_ONCE()
                            is_software_event()
                            ctx_group_list() <struct list_head *ctx_group_list (struct perf_event *event, struct perf_event_context *ctx) at core.c:1353>:
                            list_add_tail()
                            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                            list_add_rcu()
                        perf_group_attach() <void perf_group_attach (struct perf_event *event) at core.c:1520>:
                            WARN_ON_ONCE()
                            is_software_event()
                            list_add_tail()
                            perf_event__header_size() <void perf_event__header_size (struct perf_event *event) at core.c:1465>:
                            list_for_each_entry()
                    raw_spin_unlock_irqrestore()
                    list_add_tail()
                IS_ERR()
                PTR_ERR()
                list_for_each_entry()
        raw_spin_lock_irqsave()
        raw_spin_unlock_irqrestore()
        get_ctx() <void get_ctx (struct perf_event_context *ctx) at core.c:1015>:
            WARN_ON()
            atomic_inc_not_zero()
        mutex_unlock()
        perf_unpin_context() <void perf_unpin_context (struct perf_event_context *ctx) at core.c:1268>:
            raw_spin_lock_irqsave()
            raw_spin_unlock_irqrestore()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 1793)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
    perf_event_free_task() <void perf_event_free_task (struct task_struct *task) at core.c:8968>:
        for_each_task_context_nr()
        mutex_lock()
        list_for_each_entry_safe()
        perf_free_event() <void perf_free_event (struct perf_event *event, struct perf_event_context *ctx) at core.c:8940>:
            WARN_ON_ONCE()
            mutex_lock()
            list_del_init()
            mutex_unlock()
            put_event() <void put_event (struct perf_event *event) at core.c:3803>:
                atomic_long_dec_and_test()
            raw_spin_lock_irq()
            perf_group_detach() <void perf_group_detach (struct perf_event *event) at core.c:1609>:
                list_del_init()
                list_empty()
                list_for_each_entry_safe()
                list_move_tail()
                WARN_ON_ONCE()
                perf_event__header_size() <void perf_event__header_size (struct perf_event *event) at core.c:1465>:
                list_for_each_entry()
            list_del_event() <void list_del_event (struct perf_event *event, struct perf_event_context *ctx) at core.c:1555>:
                WARN_ON_ONCE()
                lockdep_assert_held()
                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                list_del_rcu()
                list_del_init()
                update_group_times() <void update_group_times (struct perf_event *leader) at core.c:1343>:
                    update_event_times() <void update_event_times (struct perf_event *event) at core.c:1301>:
                        lockdep_assert_held()
                        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                        perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                            per_cpu_ptr()
                        perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                            perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                                per_cpu_ptr()
                    list_for_each_entry()
            raw_spin_unlock_irq()
            free_event() <void free_event (struct perf_event *event) at core.c:3740>:
                WARN()
                atomic_long_cmpxchg()
                atomic_long_read()
        list_empty()
        mutex_unlock()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 1845)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
perf_event_mmap() <void perf_event_mmap (struct vm_area_struct *vma) at core.c:6135>:
    atomic_read()
    start()
    perf_event_mmap_event() <void perf_event_mmap_event (struct perf_mmap_event *mmap_event) at core.c:6015>:
        kmalloc()
        file_path()
        IS_ERR()
        file_inode()
        MAJOR()
        MINOR()
        arch_vma_name()
        strlcpy()
        IS_ALIGNED()
        perf_event_aux() <void perf_event_aux (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5689>:
            perf_event_aux_task_ctx() <void perf_event_aux_task_ctx (perf_event_aux_output_cb output, void *data, struct perf_event_context *task_ctx) at core.c:5678>:
                rcu_read_lock()
                preempt_disable()
                perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                    list_for_each_entry_rcu()
                    event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                        smp_processor_id()
                        perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                            cgroup_is_descendant()
                        pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
                preempt_enable()
                rcu_read_unlock()
            rcu_read_lock()
            list_for_each_entry_rcu()
            get_cpu_ptr()
            perf_event_aux_ctx() <void perf_event_aux_ctx (struct perf_event_context *ctx, perf_event_aux_output_cb output, void *data) at core.c:5662>:
                list_for_each_entry_rcu()
                event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
                    smp_processor_id()
                    perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                        cgroup_is_descendant()
                    pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
            rcu_dereference()
            put_cpu_ptr()
            rcu_read_unlock()
        perf_event_mmap_output() <void perf_event_mmap_output (struct perf_event *event, void *data) at core.c:5963>:
            perf_event_mmap_match() <int perf_event_mmap_match (struct perf_event *event, void *data) at core.c:5952>:
            perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
            perf_output_begin()
            perf_event_pid() <u32 perf_event_pid (struct perf_event *event, struct task_struct *p) at core.c:1156>:
                task_tgid_nr_ns()
            perf_event_tid() <u32 perf_event_tid (struct perf_event *event, struct task_struct *p) at core.c:1167>:
                task_pid_nr_ns()
            perf_output_put()
            perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
            perf_output_end()
        kfree()
perf_event_output() <void perf_event_output (struct perf_event *event, struct perf_sample_data *data, struct pt_regs *regs) at core.c:5596>:
    rcu_read_lock()
    perf_prepare_sample() <void perf_prepare_sample (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event, struct pt_regs *regs) at core.c:5490>:
        perf_misc_flags()
        perf_instruction_pointer()
        perf_callchain()
        round_up()
        perf_sample_regs_user() <void perf_sample_regs_user (struct perf_regs *regs_user, struct pt_regs *regs, struct pt_regs *regs_user_copy) at core.c:5048>:
            user_mode()
            perf_reg_abi()
            perf_get_regs_user()
        hweight64()
        perf_sample_ustack_size() <u16 perf_sample_ustack_size (u16 stack_size, u16 header_size, struct pt_regs *regs) at core.c:5089>:
            min()
            perf_ustack_task_size() <u64 perf_ustack_task_size (struct pt_regs *regs) at core.c:5078>:
                perf_user_stack_pointer()
            round_up()
        perf_sample_regs_intr() <void perf_sample_regs_intr (struct perf_regs *regs_intr, struct pt_regs *regs) at core.c:5063>:
            perf_reg_abi()
    perf_output_begin()
    perf_output_sample() <void perf_output_sample (struct perf_output_handle *handle, struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5330>:
        perf_output_put()
        perf_output_read() <void perf_output_read (struct perf_output_handle *handle, struct perf_event *event) at core.c:5306>:
            calc_timer_values() <void calc_timer_values (struct perf_event *event, u64 *now, u64 *enabled, u64 *running) at core.c:4397>:
                perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                    local_clock()
            perf_output_read_group() <void perf_output_read_group (struct perf_output_handle *handle, struct perf_event *event, u64 enabled, u64 running) at core.c:5262>:
                perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
                    count()
                primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
                list_for_each_entry()
            perf_output_read_one() <void perf_output_read_one (struct perf_output_handle *handle, struct perf_event *event, u64 enabled, u64 running) at core.c:5236>:
                perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
                    count()
                atomic64_read()
                primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
        round_up()
        perf_output_copy()
        perf_output_sample_regs() <void perf_output_sample_regs (struct perf_output_handle *handle, struct pt_regs *regs, u64 mask) at core.c:5034>:
            for_each_set_bit()
            perf_reg_value()
            perf_output_put()
        perf_output_sample_ustack() <void perf_output_sample_ustack (struct perf_output_handle *handle, u64 dump_size, struct pt_regs *regs) at core.c:5128>:
            perf_output_put()
            perf_user_stack_pointer()
            perf_output_skip()
        local_inc_return()
        local_sub()
        local_inc()
    perf_output_end()
    rcu_read_unlock()
perf_event_overflow() <int perf_event_overflow (struct perf_event *event, struct perf_sample_data *data, struct pt_regs *regs) at core.c:6470>:
perf_event_read_local() <u64 perf_event_read_local (struct perf_event *event) at core.c:3285>:
    local_irq_save()
    WARN_ON_ONCE()
    smp_processor_id()
    count()
    local64_read()
    local_irq_restore()
perf_event_read_value() <u64 perf_event_read_value (struct perf_event *event, u64 *enabled, u64 *running) at core.c:3923>:
    mutex_lock()
    perf_event_read() <int perf_event_read (struct perf_event *event, bool group) at core.c:3330>:
        smp_call_function_single()
        raw_spin_lock_irqsave()
        update_context_time() <void update_context_time (struct perf_event_context *ctx) at core.c:1280>:
            perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                local_clock()
        update_cgrp_time_from_event() <inline void update_cgrp_time_from_event (struct perf_event *event) at core.c:841>:
            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
            perf_cgroup_from_task()
        update_group_times() <void update_group_times (struct perf_event *leader) at core.c:1343>:
            update_event_times() <void update_event_times (struct perf_event *event) at core.c:1301>:
                lockdep_assert_held()
                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                    per_cpu_ptr()
                perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                    is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                    perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                        per_cpu_ptr()
            list_for_each_entry()
        update_event_times() <void update_event_times (struct perf_event *event) at core.c:1301>:
            lockdep_assert_held()
            is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
            perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                per_cpu_ptr()
            perf_event_time() <u64 perf_event_time (struct perf_event *event) at core.c:1288>:
                is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
                perf_cgroup_event_time() <inline u64 perf_cgroup_event_time (struct perf_event *event) at core.c:882>:
                    per_cpu_ptr()
        raw_spin_unlock_irqrestore()
    perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
        count()
    atomic64_read()
    list_for_each_entry()
    mutex_unlock()
perf_event_refresh() <int perf_event_refresh (struct perf_event *event, int refresh) at core.c:2363>:
    perf_event_ctx_lock() <inline struct perf_event_context *perf_event_ctx_lock (struct perf_event *event) at core.c:1125>:
        perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
            rcu_read_lock()
            ACCESS_ONCE()
            atomic_inc_not_zero()
            rcu_read_unlock()
            mutex_lock_nested()
            mutex_unlock()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                atomic_dec_and_test()
                put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2012)
                put_task_struct()
                call_rcu()
                free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                    container_of()
                    kfree()
    perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
        mutex_unlock()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2022)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
perf_event_release_kernel() <int perf_event_release_kernel (struct perf_event *event) at core.c:3816>:
    WARN_ON_ONCE()
    is_kernel_event() <bool is_kernel_event (struct perf_event *event) at core.c:164>:
        READ_ONCE()
    perf_remove_from_owner() <void perf_remove_from_owner (struct perf_event *event) at core.c:3755>:
        rcu_read_lock()
        lockless_dereference()
        get_task_struct()
        rcu_read_unlock()
        mutex_lock_nested()
        list_del_init()
        smp_store_release()
        mutex_unlock()
        put_task_struct()
    perf_event_ctx_lock() <inline struct perf_event_context *perf_event_ctx_lock (struct perf_event *event) at core.c:1125>:
        perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
            rcu_read_lock()
            ACCESS_ONCE()
            atomic_inc_not_zero()
            rcu_read_unlock()
            mutex_lock_nested()
            mutex_unlock()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                atomic_dec_and_test()
                put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2052)
                put_task_struct()
                call_rcu()
                free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                    container_of()
                    kfree()
    perf_remove_from_context() <void perf_remove_from_context (struct perf_event *event, unsigned long flags) at core.c:1785>:
        lockdep_assert_held()
        event_function_call() <void event_function_call (struct perf_event *event, event_f func, void *data) at core.c:255>:
            READ_ONCE()
            lockdep_assert_held()
            cpu_function_call() <int cpu_function_call (int cpu, remote_function_f func, void *info) at core.c:126>:
                smp_call_function_single()
                remote_function() <void remote_function (void *data) at core.c:61>:
                    task_cpu()
                    smp_processor_id()
            event_function() <int event_function (void *info) at core.c:197>:
                WARN_ON_ONCE()
                irqs_disabled()
                perf_ctx_lock() <void perf_ctx_lock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:146>:
                    raw_spin_lock()
                perf_ctx_unlock() <void perf_ctx_unlock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:154>:
                    raw_spin_unlock()
            task_function_call() <int task_function_call (struct task_struct *p, remote_function_f func, void *info) at core.c:98>:
                smp_call_function_single()
                task_cpu()
                remote_function() <void remote_function (void *data) at core.c:61>:
                    task_cpu()
                    smp_processor_id()
            raw_spin_lock_irq()
            raw_spin_unlock_irq()
    raw_spin_lock_irq()
    raw_spin_unlock_irq()
    perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
        mutex_unlock()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2089)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
    mutex_lock()
    list_for_each_entry()
    lockless_dereference()
    get_ctx() <void get_ctx (struct perf_event_context *ctx) at core.c:1015>:
        WARN_ON()
        atomic_inc_not_zero()
    mutex_unlock()
    list_first_entry_or_null()
    list_del()
    free_event() <void free_event (struct perf_event *event) at core.c:3740>:
        WARN()
        atomic_long_cmpxchg()
        atomic_long_read()
    put_event() <void put_event (struct perf_event *event) at core.c:3803>:
        atomic_long_dec_and_test()
    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
        atomic_dec_and_test()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2112)
        put_task_struct()
        call_rcu()
        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
            container_of()
            kfree()
perf_event_sysfs_show() <ssize_t perf_event_sysfs_show (struct device *dev, struct device_attribute *attr, char *page) at core.c:9439>:
    container_of()
perf_event_task_disable() <int perf_event_task_disable (void) at core.c:4370>:
    mutex_lock()
    list_for_each_entry()
    perf_event_ctx_lock() <inline struct perf_event_context *perf_event_ctx_lock (struct perf_event *event) at core.c:1125>:
        perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
            rcu_read_lock()
            ACCESS_ONCE()
            atomic_inc_not_zero()
            rcu_read_unlock()
            mutex_lock_nested()
            mutex_unlock()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                atomic_dec_and_test()
                put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2134)
                put_task_struct()
                call_rcu()
                free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                    container_of()
                    kfree()
    perf_event_for_each_child() <void perf_event_for_each_child (struct perf_event *event, void (*func) (struct perf_event *)) at core.c:4154>:
        WARN_ON_ONCE()
        mutex_lock()
        list_for_each_entry()
        mutex_unlock()
    perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
        mutex_unlock()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2149)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
    mutex_unlock()
perf_event_task_enable() <int perf_event_task_enable (void) at core.c:4354>:
    mutex_lock()
    list_for_each_entry()
    perf_event_ctx_lock() <inline struct perf_event_context *perf_event_ctx_lock (struct perf_event *event) at core.c:1125>:
        perf_event_ctx_lock_nested() <struct perf_event_context *perf_event_ctx_lock_nested (struct perf_event *event, int nesting) at core.c:1101>:
            rcu_read_lock()
            ACCESS_ONCE()
            atomic_inc_not_zero()
            rcu_read_unlock()
            mutex_lock_nested()
            mutex_unlock()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
                atomic_dec_and_test()
                put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2169)
                put_task_struct()
                call_rcu()
                free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                    container_of()
                    kfree()
    perf_event_for_each_child() <void perf_event_for_each_child (struct perf_event *event, void (*func) (struct perf_event *)) at core.c:4154>:
        WARN_ON_ONCE()
        mutex_lock()
        list_for_each_entry()
        mutex_unlock()
    perf_event_ctx_unlock() <void perf_event_ctx_unlock (struct perf_event *event, struct perf_event_context *ctx) at core.c:1130>:
        mutex_unlock()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
            atomic_dec_and_test()
            put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2184)
            put_task_struct()
            call_rcu()
            free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
                container_of()
                kfree()
    mutex_unlock()
perf_event_task_tick() <void perf_event_task_tick (void) at core.c:3126>:
    this_cpu_ptr()
    WARN_ON()
    irqs_disabled()
    list_for_each_entry_safe()
    perf_adjust_freq_unthr_context() <void perf_adjust_freq_unthr_context (struct perf_event_context *ctx, int needs_unthr) at core.c:2996>:
        raw_spin_lock()
        perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
            this_cpu_ptr()
        list_for_each_entry_rcu()
        event_filter_match() <inline int event_filter_match (struct perf_event *event) at core.c:1669>:
            smp_processor_id()
            perf_cgroup_match() <inline bool perf_cgroup_match (struct perf_event *event) at core.c:823>:
                cgroup_is_descendant()
            pmu_filter_match() <inline int pmu_filter_match (struct perf_event *event) at core.c:1662>
        perf_log_throttle() <void perf_log_throttle (struct perf_event *event, int enable) at core.c:6325>:
            perf_event_clock() <inline u64 perf_event_clock (struct perf_event *event) at core.c:510>
            primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
            perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
            perf_output_begin()
            perf_output_put()
            perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
            perf_output_end()
        start()
        stop()
        local64_read()
        perf_adjust_period() <void perf_adjust_period (struct perf_event *event, u64 nsec, u64 count, bool disable) at core.c:2962>:
            perf_calculate_period() <u64 perf_calculate_period (struct perf_event *event, u64 nsec, u64 count) at core.c:2886>:
                fls64()
                REDUCE_FLS()
                div64_u64()
            local64_read()
            stop()
            local64_set()
            start()
        perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
            this_cpu_ptr()
        raw_spin_unlock()
perf_event_update_userpage() <void perf_event_update_userpage (struct perf_event *event) at core.c:4442>:
    rcu_read_lock()
    rcu_dereference()
    calc_timer_values() <void calc_timer_values (struct perf_event *event, u64 *now, u64 *enabled, u64 *running) at core.c:4397>:
        perf_clock() <inline u64 perf_clock (void) at core.c:505>:
            local_clock()
    preempt_disable()
    barrier()
    perf_event_index() <int perf_event_index (struct perf_event *event) at core.c:4386>:
        event_idx()
    perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
        count()
    local64_read()
    atomic64_read()
    arch_perf_update_userpage() <void __weak arch_perf_update_userpage (struct perf_event *event, struct perf_event_mmap_page *userpg, u64 now) at core.c:4432>
    preempt_enable()
    rcu_read_unlock()
perf_event_wakeup() <void perf_event_wakeup (struct perf_event *event) at core.c:4976>:
    ring_buffer_wakeup() <void ring_buffer_wakeup (struct perf_event *event) at core.c:4573>:
        rcu_read_lock()
        rcu_dereference()
        list_for_each_entry_rcu()
        wake_up_all()
        rcu_read_unlock()
    kill_fasync()
    perf_event_fasync() <inline struct fasync_struct **perf_event_fasync (struct perf_event *event) at core.c:4968>
perf_log_lost_samples() <void perf_log_lost_samples (struct perf_event *event, u64 lost) at core.c:6206>:
    perf_event_header__init_id() <void perf_event_header__init_id (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5196>:
    perf_output_begin()
    perf_output_put()
    perf_event__output_id_sample() <void perf_event__output_id_sample (struct perf_event *event, struct perf_output_handle *handle, struct perf_sample_data *sample) at core.c:5228>:
    perf_output_end()
perf_output_sample() <void perf_output_sample (struct perf_output_handle *handle, struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event) at core.c:5330>:
    perf_output_put()
    perf_output_read() <void perf_output_read (struct perf_output_handle *handle, struct perf_event *event) at core.c:5306>:
        calc_timer_values() <void calc_timer_values (struct perf_event *event, u64 *now, u64 *enabled, u64 *running) at core.c:4397>:
            perf_clock() <inline u64 perf_clock (void) at core.c:505>:
                local_clock()
        perf_output_read_group() <void perf_output_read_group (struct perf_output_handle *handle, struct perf_event *event, u64 enabled, u64 running) at core.c:5262>:
            perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
                count()
            primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
            list_for_each_entry()
        perf_output_read_one() <void perf_output_read_one (struct perf_output_handle *handle, struct perf_event *event, u64 enabled, u64 running) at core.c:5236>:
            perf_event_count() <inline u64 perf_event_count (struct perf_event *event) at core.c:3269>:
                count()
            atomic64_read()
            primary_event_id() <u64 primary_event_id (struct perf_event *event) at core.c:1182>:
    round_up()
    perf_output_copy()
    perf_output_sample_regs() <void perf_output_sample_regs (struct perf_output_handle *handle, struct pt_regs *regs, u64 mask) at core.c:5034>:
        for_each_set_bit()
        perf_reg_value()
        perf_output_put()
    perf_output_sample_ustack() <void perf_output_sample_ustack (struct perf_output_handle *handle, u64 dump_size, struct pt_regs *regs) at core.c:5128>:
        perf_output_put()
        perf_user_stack_pointer()
        perf_output_skip()
    local_inc_return()
    local_sub()
    local_inc()
perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
    this_cpu_ptr()
perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
    this_cpu_ptr()
perf_pmu_migrate_context() <void perf_pmu_migrate_context (struct pmu *pmu, int src_cpu, int dst_cpu) at core.c:8702>:
    LIST_HEAD()
    per_cpu_ptr()
    mutex_lock_double() <void mutex_lock_double (struct mutex *a, struct mutex *b) at core.c:8214>:
        swap()
        mutex_lock()
        mutex_lock_nested()
    list_for_each_entry_safe()
    perf_remove_from_context() <void perf_remove_from_context (struct perf_event *event, unsigned long flags) at core.c:1785>:
        lockdep_assert_held()
        event_function_call() <void event_function_call (struct perf_event *event, event_f func, void *data) at core.c:255>:
            READ_ONCE()
            lockdep_assert_held()
            cpu_function_call() <int cpu_function_call (int cpu, remote_function_f func, void *info) at core.c:126>:
                smp_call_function_single()
                remote_function() <void remote_function (void *data) at core.c:61>:
                    task_cpu()
                    smp_processor_id()
            event_function() <int event_function (void *info) at core.c:197>:
                WARN_ON_ONCE()
                irqs_disabled()
                perf_ctx_lock() <void perf_ctx_lock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:146>:
                    raw_spin_lock()
                perf_ctx_unlock() <void perf_ctx_unlock (struct perf_cpu_context *cpuctx, struct perf_event_context *ctx) at core.c:154>:
                    raw_spin_unlock()
            task_function_call() <int task_function_call (struct task_struct *p, remote_function_f func, void *info) at core.c:98>:
                smp_call_function_single()
                task_cpu()
                remote_function() <void remote_function (void *data) at core.c:61>:
                    task_cpu()
                    smp_processor_id()
            raw_spin_lock_irq()
            raw_spin_unlock_irq()
    unaccount_event_cpu() <void unaccount_event_cpu (struct perf_event *event, int cpu) at core.c:3558>:
        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
        atomic_dec()
        per_cpu()
    put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (R):
        atomic_dec_and_test()
        put_ctx() <void put_ctx (struct perf_event_context *ctx) at core.c:1029> (recursive: see 2334)
        put_task_struct()
        call_rcu()
        free_ctx() <void free_ctx (struct rcu_head *head) at core.c:1020>:
            container_of()
            kfree()
    list_add()
    synchronize_rcu()
    list_del()
    account_event_cpu() <void account_event_cpu (struct perf_event *event, int cpu) at core.c:7810>:
        is_cgroup_event() <inline int is_cgroup_event (struct perf_event *event) at core.c:831>:
        atomic_inc()
        per_cpu()
    perf_install_in_context() <void perf_install_in_context (struct perf_event_context *ctx, struct perf_event *event, int cpu) at core.c:2178>:
        READ_ONCE()
        lockdep_assert_held()
        cpu_function_call() <int cpu_function_call (int cpu, remote_function_f func, void *info) at core.c:126>:
            smp_call_function_single()
            remote_function() <void remote_function (void *data) at core.c:61>:
                task_cpu()
                smp_processor_id()
        WARN_ON_ONCE()
        task_cpu()
        raw_spin_lock_irq()
        raw_spin_unlock_irq()
    get_ctx() <void get_ctx (struct perf_event_context *ctx) at core.c:1015>:
        WARN_ON()
        atomic_inc_not_zero()
    mutex_unlock()
perf_pmu_register() <int perf_pmu_register (struct pmu *pmu, const char *name, int type) at core.c:7621>:
    mutex_lock()
    alloc_percpu()
    idr_alloc()
    pmu_dev_alloc() <int pmu_dev_alloc (struct pmu *pmu) at core.c:7589>:
        kzalloc()
        device_initialize()
        dev_set_name()
        dev_set_drvdata()
        pmu_dev_release() <void pmu_dev_release (struct device *dev) at core.c:7584>:
            kfree()
        device_add()
        put_device()
    find_pmu_context() <struct perf_cpu_context __percpu *find_pmu_context (int ctxn) at core.c:7461>:
        list_for_each_entry()
    for_each_possible_cpu()
    per_cpu_ptr()
    lockdep_set_class()
    perf_pmu_start_txn() <void perf_pmu_start_txn (struct pmu *pmu, unsigned int flags) at core.c:7417>:
        perf_pmu_disable() <void perf_pmu_disable (struct pmu *pmu) at core.c:973>:
            this_cpu_ptr()
    perf_pmu_commit_txn() <int perf_pmu_commit_txn (struct pmu *pmu) at core.c:7427>:
        perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
            this_cpu_ptr()
    perf_pmu_cancel_txn() <void perf_pmu_cancel_txn (struct pmu *pmu) at core.c:7440>:
        perf_pmu_enable() <void perf_pmu_enable (struct pmu *pmu) at core.c:980>:
            this_cpu_ptr()
    perf_pmu_nop_txn() <void perf_pmu_nop_txn (struct pmu *pmu, unsigned int flags) at core.c:7406>
    perf_pmu_nop_int() <int perf_pmu_nop_int (struct pmu *pmu) at core.c:7410>
    perf_pmu_nop_void() <void perf_pmu_nop_void (struct pmu *pmu) at core.c:7402>
    perf_event_idx_default() <int perf_event_idx_default (struct perf_event *event) at core.c:7452>
    list_add_rcu()
    atomic_set()
    mutex_unlock()
    device_del()
    put_device()
    idr_remove()
    free_percpu()
perf_pmu_unregister() <void perf_pmu_unregister (struct pmu *pmu) at core.c:7723>:
    mutex_lock()
    list_del_rcu()
    mutex_unlock()
    synchronize_srcu()
    synchronize_rcu()
    free_percpu()
    idr_remove()
    device_del()
    put_device()
    free_pmu_context() <void free_pmu_context (struct pmu *pmu) at core.c:7490>:
        mutex_lock()
        list_for_each_entry()
        update_pmu_context() <void update_pmu_context (struct pmu *pmu, struct pmu *old_pmu) at core.c:7476>:
            for_each_possible_cpu()
            per_cpu_ptr()
        free_percpu()
        mutex_unlock()
perf_prepare_sample() <void perf_prepare_sample (struct perf_event_header *header, struct perf_sample_data *data, struct perf_event *event, struct pt_regs *regs) at core.c:5490>:
    perf_misc_flags()
    perf_instruction_pointer()
    perf_callchain()
    round_up()
    perf_sample_regs_user() <void perf_sample_regs_user (struct perf_regs *regs_user, struct pt_regs *regs, struct pt_regs *regs_user_copy) at core.c:5048>:
        user_mode()
        perf_reg_abi()
        perf_get_regs_user()
    hweight64()
    perf_sample_ustack_size() <u16 perf_sample_ustack_size (u16 stack_size, u16 header_size, struct pt_regs *regs) at core.c:5089>:
        min()
        perf_ustack_task_size() <u64 perf_ustack_task_size (struct pt_regs *regs) at core.c:5078>:
            perf_user_stack_pointer()
        round_up()
    perf_sample_regs_intr() <void perf_sample_regs_intr (struct perf_regs *regs_intr, struct pt_regs *regs) at core.c:5063>:
        perf_reg_abi()
perf_proc_update_handler() <int perf_proc_update_handler (struct ctl_table *table, int write, void __user *buffer, size_t *lenp, loff_t *ppos) at core.c:385>:
    proc_dointvec_minmax()
    DIV_ROUND_UP()
    update_perf_cpu_limits() <void update_perf_cpu_limits (void) at core.c:374>:
        do_div()
        ACCESS_ONCE()
perf_register_guest_info_callbacks() <int perf_register_guest_info_callbacks (struct perf_guest_info_callbacks *cbs) at core.c:5019>:
perf_sample_event_took() <void perf_sample_event_took (u64 sample_len_ns) at core.c:444>:
    ACCESS_ONCE()
    DIV_ROUND_UP()
    update_perf_cpu_limits() <void update_perf_cpu_limits (void) at core.c:374>:
        do_div()
        ACCESS_ONCE()
    irq_work_queue()
    early_printk()
perf_sched_cb_dec() <void perf_sched_cb_dec (struct pmu *pmu) at core.c:2612>:
    this_cpu_dec()
perf_sched_cb_inc() <void perf_sched_cb_inc (struct pmu *pmu) at core.c:2617>:
    this_cpu_inc()
perf_swevent_get_recursion_context() <int perf_swevent_get_recursion_context (void) at core.c:6685>:
    this_cpu_ptr()
    get_recursion_context()
perf_swevent_put_recursion_context() <inline void perf_swevent_put_recursion_context (int rctx) at core.c:6693>:
    this_cpu_ptr()
    put_recursion_context()
perf_swevent_set_period() <u64 perf_swevent_set_period (struct perf_event *event) at core.c:6499>:
    local64_read()
    div64_u64()
    local64_cmpxchg()
perf_tp_event() <void perf_tp_event (u64 addr, u64 count, void *record, int entry_size, struct pt_regs *regs, struct hlist_head *head, int rctx, struct task_struct *task) at core.c:6956>:
    perf_sample_data_init()
    hlist_for_each_entry_rcu()
    perf_tp_event_match() <int perf_tp_event_match (struct perf_event *event, struct perf_sample_data *data, struct pt_regs *regs) at core.c:6938>:
        perf_tp_filter_match() <int perf_tp_filter_match (struct perf_event *event, struct perf_sample_data *data) at core.c:6924>:
            likely()
            filter_match_preds()
    perf_swevent_event() <void perf_swevent_event (struct perf_event *event, u64 nr, struct perf_sample_data *data, struct pt_regs *regs) at core.c:6548>:
        local64_add()
        is_sampling_event()
        perf_swevent_overflow() <void perf_swevent_overflow (struct perf_event *event, u64 overflow, struct perf_sample_data *data, struct pt_regs *regs) at core.c:6522>:
            perf_swevent_set_period() <u64 perf_swevent_set_period (struct perf_event *event) at core.c:6499>:
                local64_read()
                div64_u64()
                local64_cmpxchg()
        local64_add_negative()
    rcu_read_lock()
    rcu_dereference()
    list_for_each_entry_rcu()
    rcu_read_unlock()
    perf_swevent_put_recursion_context() <inline void perf_swevent_put_recursion_context (int rctx) at core.c:6693>:
        this_cpu_ptr()
        put_recursion_context()
perf_unregister_guest_info_callbacks() <int perf_unregister_guest_info_callbacks (struct perf_guest_info_callbacks *cbs) at core.c:5026>:
ring_buffer_get() <struct ring_buffer *ring_buffer_get (struct perf_event *event) at core.c:4586>:
    rcu_read_lock()
    rcu_dereference()
    atomic_inc_not_zero()
    rcu_read_unlock()
ring_buffer_put() <void ring_buffer_put (struct ring_buffer *rb) at core.c:4601>:
    atomic_dec_and_test()
    WARN_ON_ONCE()
    list_empty()
    call_rcu()
